Directory structure:
└── code/
    ├── 01_prepare_huc_list.sh
    ├── 02_create_chunks.sh
    ├── 03_submit_job.sh
    ├── 03_verify_output.sh
    ├── clip_dem_by_huc12.sh
    ├── download.sh
    ├── download_wbd_zips.sh
    ├── extract_wbd.sh
    ├── prepare_data.sh
    ├── process_chunk.sbatch
    ├── submit_clip_array.sh
    └── .ipynb_checkpoints/
        └── download_wbd_zips-checkpoint.sh

================================================
File: 01_prepare_huc_list.sh
================================================
#!/usr/bin/env bash

# -------------------------------------------------------------------
# Script: 01_setup_and_generate_list.sh
# Purpose: Prepares directories and generates the master list of HUC12 IDs.
#          Run this script ONCE from the command line.
# -------------------------------------------------------------------

set -e # Exit immediately if a command exits with a non-zero status.

# === CONFIGURATION ===
PROCESSED_DIR="/scratch/kdahal3/DEM_CONUS/processed"
MERGED_HUC_FILE="${PROCESSED_DIR}/WBD_CONUS_HUC12.gpkg"
CLIPPED_DEM_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems"
HUC_LAYER_NAME="WBDHU12"
HUC_ID_FIELD="huc12"

# This is the master list that the Slurm job array will read from.
HUC_LIST_FILE="${CLIPPED_DEM_DIR}/huc12_list.txt"

# === SETUP ===
echo "Creating output directory: ${CLIPPED_DEM_DIR}"
mkdir -p "${CLIPPED_DEM_DIR}"

echo "-------------------------------------------------------------"
echo "Generating list of all HUC12 IDs..."
echo "-------------------------------------------------------------"

# === GENERATE HUC LIST ===
# This is the same command from your original script.
ogrinfo -ro -q -sql "SELECT ${HUC_ID_FIELD} FROM ${HUC_LAYER_NAME}" "${MERGED_HUC_FILE}" | \
grep "${HUC_ID_FIELD} (String)" | awk -F' = ' '{print $2}' > "${HUC_LIST_FILE}"

NUM_HUCS=$(wc -l < "${HUC_LIST_FILE}")

if [ "${NUM_HUCS}" -eq 0 ]; then
    echo "[ERROR] HUC list is empty. Check your ogrinfo command and file paths."
    exit 1
fi

echo "✅ Found ${NUM_HUCS} HUC12 basins to process."
echo "List saved to: ${HUC_LIST_FILE}"
echo ""
echo "-------------------------------------------------------------"
echo "Setup is complete. You can now submit the job array."
echo "Next steps:"
echo "1. Create a log directory: mkdir -p logs"
echo "2. Submit the job: sbatch submit_clip_array.sbatch"
echo "-------------------------------------------------------------"


================================================
File: 02_create_chunks.sh
================================================
#!/usr/bin/env bash

# -------------------------------------------------------------------
# Script: 02_create_chunks.sh
# Purpose: Splits the master HUC12 list into smaller chunk files
#          for batch processing.
# -------------------------------------------------------------------

set -e

# === CONFIGURATION ===
CLIPPED_DEM_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems"
HUC_LIST_FILE="${CLIPPED_DEM_DIR}/huc12_list.txt"
CHUNK_DIR="${CLIPPED_DEM_DIR}/huc_chunks"
HUCS_PER_CHUNK=100

# === SETUP ===
if [ ! -f "${HUC_LIST_FILE}" ]; then
    echo "[ERROR] Master HUC list not found at: ${HUC_LIST_FILE}"
    exit 1
fi

echo "Cleaning old chunks and creating new chunk directory..."
rm -rf "${CHUNK_DIR}"
mkdir -p "${CHUNK_DIR}"

echo "Splitting master list into chunks of ${HUCS_PER_CHUNK} HUCs each..."

# THE FIX IS HERE: Removed '--numeric-suffixes=1'.
# The default behavior is to start numbering from 0000.
split -l "${HUCS_PER_CHUNK}" -a 4 --numeric-suffixes --additional-suffix=.txt \
    "${HUC_LIST_FILE}" "${CHUNK_DIR}/chunk_"

NUM_CHUNKS=$(find "${CHUNK_DIR}" -type f | wc -l)

echo "-------------------------------------------------------------"
echo "✅ Split HUC list into ${NUM_CHUNKS} chunk files (named chunk_0000.txt, etc.)."
echo "Chunks are located in: ${CHUNK_DIR}"
echo "You can now submit the job with 'sbatch submit_clip_by_chunk.sbatch'"
echo "-------------------------------------------------------------"


================================================
File: 03_submit_job.sh
================================================
#!/bin/bash

# -------------------------------------------------------------------
# Script: 03_submit_job.sh
# Purpose: Calculates the number of chunks and submits the job array
#          to Slurm with the correct parameters.
# -------------------------------------------------------------------

set -e

CHUNK_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems/huc_chunks"

echo "Finding chunk files in ${CHUNK_DIR}..."

# 1. Calculate the number of chunks
NUM_CHUNKS=$(find "${CHUNK_DIR}" -type f | wc -l)

if [ "${NUM_CHUNKS}" -eq 0 ]; then
    echo "[ERROR] No chunk files found. Did you run 02_create_chunks.sh?"
    exit 1
fi

# 2. Calculate the array index range (0 to N-1)
ARRAY_RANGE="0-$((NUM_CHUNKS - 1))"

echo "Found ${NUM_CHUNKS} chunks. Submitting job array with range: ${ARRAY_RANGE}"

# 3. Submit the job, passing the array range on the command line
sbatch --array="${ARRAY_RANGE}%100" process_chunk.sbatch

echo "✅ Job submitted."



================================================
File: 03_verify_output.sh
================================================
#!/usr/bin/env bash

# -------------------------------------------------------------------
# Script: 03_verify_output.sh
# Purpose: Verifies the output of the Slurm job array.
# -------------------------------------------------------------------

set -e

# === CONFIGURATION ===
CLIPPED_DEM_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems"
HUC_LIST_FILE="${CLIPPED_DEM_DIR}/huc12_list.txt"

echo "Verifying output..."

# Check if the list file exists
if [ ! -f "${HUC_LIST_FILE}" ]; then
    echo "[ERROR] HUC list file not found at ${HUC_LIST_FILE}. Cannot verify."
    exit 1
fi

NUM_HUCS=$(wc -l < "${HUC_LIST_FILE}")
NUM_OUTPUTS=$(find "${CLIPPED_DEM_DIR}" -name "*.tif" | wc -l)

echo "-------------------------------------------------------------"
echo "Verification Results:"
echo "Expected DEMs: ${NUM_HUCS}"
echo "Generated DEMs:  ${NUM_OUTPUTS}"
echo "-------------------------------------------------------------"

if [ "${NUM_OUTPUTS}" -lt "${NUM_HUCS}" ]; then
    echo "[WARN] Not all DEMs were generated."
    echo "You can re-run the 'sbatch submit_clip.sbatch' command."
    echo "It will automatically skip the ones that are already complete."
else
    echo "✅ Success! All expected DEMs have been generated."
fi



================================================
File: clip_dem_by_huc12.sh
================================================
#!/usr/bin/env bash

# -------------------------------------------------------------------
# Script: clip_dem_by_huc12.sh (Revised)
# Purpose: Clips a master DEM VRT to each HUC12 boundary in parallel.
# -------------------------------------------------------------------

set -e # Exit immediately if a command exits with a non-zero status.

# === CONFIGURATION ===
PROCESSED_DIR="/scratch/kdahal3/DEM_CONUS/processed"
MERGED_HUC_FILE="${PROCESSED_DIR}/WBD_CONUS_HUC12.gpkg"
DEM_VRT_FILE="${PROCESSED_DIR}/CONUS_DEM_30m.vrt"
CLIPPED_DEM_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems"
THREADS=126
HUC_LAYER_NAME="WBDHU12"
HUC_ID_FIELD="huc12"

# === SETUP ===
mkdir -p "${CLIPPED_DEM_DIR}"
TMPDIR="${CLIPPED_DEM_DIR}/tmp"
mkdir -p "${TMPDIR}"

echo "-------------------------------------------------------------"
echo "Clipping DEM for each HUC12 basin"
echo "-------------------------------------------------------------"

# === STEP 1: GET LIST OF ALL HUC12 IDs ===
HUC_LIST_FILE="${TMPDIR}/huc12_list.txt"
echo "[1/4] Generating list of all HUC12 IDs..."
ogrinfo -ro -q -sql "SELECT ${HUC_ID_FIELD} FROM ${HUC_LAYER_NAME}" "${MERGED_HUC_FILE}" | \
grep "${HUC_ID_FIELD} (String)" | awk -F' = ' '{print $2}' > "${HUC_LIST_FILE}"

NUM_HUCS=$(wc -l < "${HUC_LIST_FILE}")
echo "[INFO] Found ${NUM_HUCS} HUC12 basins to process."

# === STEP 2: PERFORM A DRY RUN FOR A SINGLE HUC ===
echo "[2/4] Performing a dry run to verify the command..."
TEST_HUC_ID=$(head -n 1 "${HUC_LIST_FILE}")
if [ -z "${TEST_HUC_ID}" ]; then
    echo "[ERROR] HUC list is empty. Cannot perform dry run."
    exit 1
fi

echo "A test command will be generated for the first HUC: ${TEST_HUC_ID}"
echo "The command that will be run in parallel is:"
echo "-------------------------------------------------------------"
# Note the 'echo' at the beginning. This just prints the command.
echo gdalwarp \
    -q \
    -cutline "'${MERGED_HUC_FILE}'" \
    -cl "'${HUC_LAYER_NAME}'" \
    -cwhere "'${HUC_ID_FIELD} = '\''${TEST_HUC_ID}'\''" \
    -crop_to_cutline \
    -dstnodata -9999 \
    -co "COMPRESS=LZW" -co "PREDICTOR=2" \
    "'${DEM_VRT_FILE}'" \
    "'${CLIPPED_DEM_DIR}/${TEST_HUC_ID}.tif'"
echo "-------------------------------------------------------------"
read -p "Does this command look correct? (y/n) " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Aborting script."
    exit 1
fi

# === STEP 3: DEFINE AND RUN THE PARALLEL PROCESSING ===
echo "[3/4] Starting parallel processing (${THREADS} jobs)..."

# Define the function to be used by GNU Parallel
process_huc12() {
    local HUC12_ID="$1"
    # Re-state variables inside function for clarity
    local MERGED_HUC_FILE="/scratch/kdahal3/DEM_CONUS/processed/WBD_CONUS_HUC12.gpkg"
    local DEM_VRT_FILE="/scratch/kdahal3/DEM_CONUS/processed/CONUS_DEM_30m.vrt"
    local CLIPPED_DEM_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems"
    local HUC_LAYER_NAME="WBDHU12"
    local HUC_ID_FIELD="huc12"
    local OUT_RASTER="${CLIPPED_DEM_DIR}/${HUC12_ID}.tif"

    # Skip if the output file already exists to allow resuming
    [ -f "${OUT_RASTER}" ] && return

    gdalwarp \
        -q \
        -cutline "${MERGED_HUC_FILE}" \
        -cl "${HUC_LAYER_NAME}" \
        -cwhere "${HUC_ID_FIELD} = '${HUC12_ID}'" \
        -crop_to_cutline \
        -dstnodata -9999 \
        -co "COMPRESS=LZW" -co "PREDICTOR=2" \
        "${DEM_VRT_FILE}" \
        "${OUT_RASTER}"
}

# Export the function so it's available to parallel's sub-shells
export -f process_huc12

# Run the job
cat "${HUC_LIST_FILE}" | parallel -j ${THREADS} --eta --bar "process_huc12 {}"

# === STEP 4: VERIFY OUTPUT ===
echo "[4/4] Verifying output..."
NUM_OUTPUTS=$(find "${CLIPPED_DEM_DIR}" -name "*.tif" | wc -l)
echo "[INFO] Produced ${NUM_OUTPUTS} / ${NUM_HUCS} clipped DEMs."

if [ "${NUM_OUTPUTS}" -lt "${NUM_HUCS}" ]; then
    echo "[WARN] Not all DEMs were generated. You can re-run the script to process the missing ones."
fi

echo "-------------------------------------------------------------"
echo "✅ All HUC12 basins processed successfully."
echo "Output located in: ${CLIPPED_DEM_DIR}"
echo "-------------------------------------------------------------"


================================================
File: download.sh
================================================
#!/usr/bin/env bash
# -------------------------------------------------------------
# Script: download_conus_copdem30.sh
# Purpose: Download Copernicus DEM GLO-30 (30m) tiles for CONUS
# Source: https://registry.opendata.aws/copernicus-dem
# Managed by: Sinergise
# License: Copernicus Open Data License
# -------------------------------------------------------------

# === CONFIGURATION ===
OUTDIR="/scratch/kdahal3/DEM_CONUS_30m"   # output directory
THREADS=126                                # number of parallel downloads
BUCKET="s3://copernicus-dem-30m"
TMPDIR="${OUTDIR}/tmp"

# Approximate bounding box for CONUS
LAT_MIN=24
LAT_MAX=50
LON_MIN=-125
LON_MAX=-66

# === SETUP ===
mkdir -p "${OUTDIR}" "${TMPDIR}"
echo "-------------------------------------------------------------"
echo "Downloading Copernicus GLO-30 DEM for CONUS"
echo "Threads: ${THREADS}"
echo "Output Directory: ${OUTDIR}"
echo "-------------------------------------------------------------"
sleep 2

# === STEP 1: LIST ALL TILES ===
echo "[1/4] Listing all available tiles from S3..."
aws s3 ls --no-sign-request ${BUCKET}/ --recursive | grep '.tif' > "${TMPDIR}/all_tiles.txt"

# === STEP 2: FILTER TILES BY LAT/LON ===
echo "[2/4] Filtering tiles within CONUS bounds..."
awk -v latmin=$LAT_MIN -v latmax=$LAT_MAX -v lonmin=$LON_MIN -v lonmax=$LON_MAX '
{
  match($4, /N([0-9]{2})_00_W([0-9]{3})_00|N([0-9]{2})_00_E([0-9]{3})_00|S([0-9]{2})_00_W([0-9]{3})_00|S([0-9]{2})_00_E([0-9]{3})_00/, a)
  lat = (a[1] != "") ? a[1] : (a[3] != "") ? a[3] : (a[5] != "") ? -a[5] : -a[7]
  lon = (a[2] != "") ? -a[2] : (a[4] != "") ? a[4] : (a[6] != "") ? -a[6] : a[8]
  if (lat >= latmin && lat <= latmax && lon >= lonmin && lon <= lonmax)
    print $4
}' "${TMPDIR}/all_tiles.txt" > "${TMPDIR}/conus_tiles.txt"

NUM_TILES=$(wc -l < "${TMPDIR}/conus_tiles.txt")
echo "[INFO] Found ${NUM_TILES} tiles covering CONUS."

if [ "$NUM_TILES" -eq 0 ]; then
  echo "[ERROR] No tiles found. Check bounds or bucket path."
  exit 1
fi

# === STEP 3: PARALLEL DOWNLOAD ===
echo "[3/4] Downloading tiles in parallel (${THREADS} threads)..."
cat "${TMPDIR}/conus_tiles.txt" | \
parallel -j ${THREADS} --eta \
"aws s3 cp --no-sign-request ${BUCKET}/{} ${OUTDIR}/{}"

echo "[INFO] Downloads complete."

# === STEP 4: VERIFY SAMPLE TILE ===
SAMPLE=$(head -n 1 "${TMPDIR}/conus_tiles.txt")
echo "[4/4] Checking sample file metadata:"
gdalinfo "${OUTDIR}/${SAMPLE}" | head -n 10 || echo "[WARN] GDAL not installed or check skipped."

echo "-------------------------------------------------------------"
echo "✅ Copernicus GLO-30 DEM for CONUS successfully downloaded."
echo "Output: ${OUTDIR}"
echo "Tiles: ${NUM_TILES}"
echo "-------------------------------------------------------------"



================================================
File: download_wbd_zips.sh
================================================
#!/bin/bash
base="https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape"
for i in $(seq -w 1 22); do
  wget -c "$base/WBD_${i}_HU2_Shape.zip"
done




================================================
File: extract_wbd.sh
================================================
#!/bin/bash
mkdir -p WBD_unzipped
for z in WBD_*.zip; do
  name=$(basename "$z" .zip)
  mkdir -p "WBD_unzipped/$name"
  unzip -o "$z" -d "WBD_unzipped/$name"
done




================================================
File: prepare_data.sh
================================================
#!/usr/bin/env bash

# -------------------------------------------------------------------
# Script: prepare_data.sh (Improved)
# Purpose: Pre-processes DEM tiles and HUC shapefiles for clipping.
#
# 1. Merges all HUC12 shapefiles into a single GeoPackage.
# 2. Creates critical attribute and spatial indexes on the GeoPackage for fast queries.
# 3. Creates a Virtual Raster (VRT) from all DEM tiles.
# -------------------------------------------------------------------

set -e # Exit immediately if a command exits with a non-zero status.

# === CONFIGURATION ===
DEM_DIR="/scratch/kdahal3/DEM_CONUS_30m"
HUC_UNZIPPED_DIR="/scratch/kdahal3/DEM_CONUS/WBD_unzipped"
OUTPUT_DIR="/scratch/kdahal3/DEM_CONUS/processed"
MERGED_HUC_FILE="${OUTPUT_DIR}/WBD_CONUS_HUC12.gpkg"
DEM_VRT_FILE="${OUTPUT_DIR}/CONUS_DEM_30m.vrt"
DEM_LIST_FILE="${OUTPUT_DIR}/dem_file_list.txt"

# === SETUP ===
mkdir -p "${OUTPUT_DIR}"
echo "-------------------------------------------------------------"
echo "Preparing data for processing..."
echo "Output directory: ${OUTPUT_DIR}"
echo "-------------------------------------------------------------"

# === PART 1: MERGE HUC12 SHAPEFILES ===
echo "[1/5] Merging HUC12 shapefiles..."

if [ -f "$MERGED_HUC_FILE" ]; then
    echo "[INFO] Merged HUC file already exists. Skipping merge."
else
    HUC12_SHAPEFILES=$(find "${HUC_UNZIPPED_DIR}" -path "*/Shape/WBDHU12.shp")
    FIRST_SHP=$(echo "$HUC12_SHAPEFILES" | head -n 1)
    
    echo "  - Creating base file from: $FIRST_SHP"
    ogr2ogr -f "GPKG" -nlt PROMOTE_TO_MULTI "${MERGED_HUC_FILE}" "${FIRST_SHP}"

    echo "$HUC12_SHAPEFILES" | tail -n +2 | while read -r shp; do
        printf "  - Appending: %s\n" "$shp"
        ogr2ogr -f "GPKG" -append -update "${MERGED_HUC_FILE}" "${shp}"
    done
    echo "[SUCCESS] Merged all HUC12 shapefiles into ${MERGED_HUC_FILE}"
fi

# === PART 2: CREATE INDEXES ON GEOPACKAGE (CRITICAL FOR PERFORMANCE) ===
echo "[2/5] Creating indexes on HUC GeoPackage..."
# This step is essential. Without indexes, querying a single HUC from the
# large file is extremely slow (minutes), causing the entire workflow to hang.

echo "  - Creating attribute index on 'huc12' column..."
ogrinfo -sql "CREATE INDEX IF NOT EXISTS idx_wbdhu12_huc12 ON WBDHU12(huc12)" "${MERGED_HUC_FILE}"

echo "  - Creating spatial index on geometry column..."
ogrinfo -sql "SELECT CreateSpatialIndex('WBDHU12', 'geom')" "${MERGED_HUC_FILE}"

echo "[SUCCESS] Indexes created successfully."

# === PART 3: GENERATE LIST OF DEM FILES ===
echo "[3/5] Generating list of primary DEM files..."
find "${DEM_DIR}" -name "*_DEM.tif" > "${DEM_LIST_FILE}"
NUM_DEMS=$(wc -l < "${DEM_LIST_FILE}")

if [ "$NUM_DEMS" -eq 0 ]; then
  echo "[ERROR] No DEM files found matching the pattern '*_DEM.tif'."
  exit 1
fi
echo "[SUCCESS] Found ${NUM_DEMS} primary DEM files."

# === PART 4: VERIFY THE FILE LIST ===
echo "[4/5] Verifying DEM file list. Here are the first 5 files found:"
echo "-------------------------------------------------------------"
head -n 5 "${DEM_LIST_FILE}"
echo "-------------------------------------------------------------"
read -p "Does this list look correct? (y/n) " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Aborting script."
    exit 1
fi

# === PART 5: BUILD DEM VIRTUAL RASTER (VRT) ===
echo "[5/5] Building Virtual Raster (VRT) from the file list..."
if [ -f "$DEM_VRT_FILE" ]; then
    echo "[INFO] DEM VRT file already exists. Overwriting."
fi
gdalbuildvrt -overwrite -input_file_list "${DEM_LIST_FILE}" "${DEM_VRT_FILE}"

if [ -f "$DEM_VRT_FILE" ]; then
    echo "[SUCCESS] Created DEM Virtual Raster at ${DEM_VRT_FILE}"
else
    echo "[ERROR] gdalbuildvrt failed to create the VRT file."
    exit 1
fi

echo "-------------------------------------------------------------"
echo "✅ Data preparation complete and optimized for performance."
echo "-------------------------------------------------------------"


================================================
File: process_chunk.sbatch
================================================
#!/bin/bash

#SBATCH --job-name=clip_dem_batch
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=4G
#SBATCH --time=00:10:00
#SBATCH --output=logs/clip_batch_%A_%a.out
#SBATCH --error=logs/clip_batch_%A_%a.err

# --- NO #SBATCH --array DIRECTIVE HERE ---
# The array range will be provided by the submitter script.

set -e

# === CONFIGURATION ===
PROCESSED_DIR="/scratch/kdahal3/DEM_CONUS/processed"
MERGED_HUC_FILE="${PROCESSED_DIR}/WBD_CONUS_HUC12.gpkg"
DEM_VRT_FILE="${PROCESSED_DIR}/CONUS_DEM_30m.vrt"
CLIPPED_DEM_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems"
CHUNK_DIR="${CLIPPED_DEM_DIR}/huc_chunks"
HUC_LAYER_NAME="WBDHU12"
HUC_ID_FIELD="huc12"

# This script still uses SLURM_ARRAY_TASK_ID, which is set by sbatch
CHUNK_FILE="${CHUNK_DIR}/chunk_$(printf "%04d" ${SLURM_ARRAY_TASK_ID}).txt"

if [ ! -f "${CHUNK_FILE}" ]; then
    echo "[ERROR] Task ${SLURM_ARRAY_TASK_ID} could not find its chunk file: ${CHUNK_FILE}"
    exit 1
fi

echo "=========================================================="
echo "Starting Slurm Batch Task: ${SLURM_ARRAY_TASK_ID}"
echo "Processing HUCs from chunk file: ${CHUNK_FILE}"
echo "Time: $(date)"
echo "=========================================================="

PROCESSED_COUNT=0
while read -r HUC12_ID; do
    [ -z "${HUC12_ID}" ] && continue
    OUT_RASTER="${CLIPPED_DEM_DIR}/${HUC12_ID}.tif"
    [ -f "${OUT_RASTER}" ] && continue

    gdalwarp \
        -q \
        -cutline "${MERGED_HUC_FILE}" \
        -cl "${HUC_LAYER_NAME}" \
        -cwhere "${HUC_ID_FIELD} = '${HUC12_ID}'" \
        -crop_to_cutline \
        -dstnodata -9999 \
        -co "COMPRESS=LZW" -co "PREDICTOR=2" \
        "${DEM_VRT_FILE}" \
        "${OUT_RASTER}"

    PROCESSED_COUNT=$((PROCESSED_COUNT + 1))
done < "${CHUNK_FILE}"

echo "----------------------------------------------------------"
echo "✅ Task ${SLURM_ARRAY_TASK_ID} complete."
echo "Processed ${PROCESSED_COUNT} new HUCs from this chunk."
echo "Finished at: $(date)"
echo "----------------------------------------------------------"


================================================
File: submit_clip_array.sh
================================================
#!/usr/bin/env bash
#
# submit_clip_array.sh
#
# Single-script workflow:
#  - Create HUC list from GeoPackage
#  - Estimate per-file runtime from historical run (default: 128 cores * 5 hours for 103000 files)
#  - Compute chunk sizes for target job length (default 20 minutes)
#  - Split HUC list into chunk files
#  - Create SLURM array sbatch script that processes each chunk
#  - Submit array with concurrency limit
#
# Usage:
#   ./submit_clip_array.sh [--minutes=20] [--cpus-per-task=4] [--concurrency=50] [--array-name=clip_huc12] [--dry-run]
#
# Example:
#   ./submit_clip_array.sh --minutes=15 --cpus-per-task=4 --concurrency=50
#

set -euo pipefail

#######  USER-CONFIGURABLE SETTINGS (change here or via CLI args) #######
# Data paths (edit if needed)
PROCESSED_DIR="/scratch/kdahal3/DEM_CONUS/processed"
MERGED_HUC_FILE="${PROCESSED_DIR}/WBD_CONUS_HUC12.gpkg"
HUC_LAYER_NAME="WBDHU12"
HUC_ID_FIELD="huc12"
DEM_VRT_FILE="${PROCESSED_DIR}/CONUS_DEM_30m.vrt"
CLIPPED_DEM_DIR="/scratch/kdahal3/DEM_CONUS/clipped_huc12_dems"

# Prior run stats (used for runtime estimate)
# You told me: used to take 128 cores and 5 hours to run all 103K files.
PRIOR_CORES=128
PRIOR_HOURS=5
PRIOR_TOTAL_FILES=103000

# Defaults for splitting / submission
TARGET_MINUTES_DEFAULT=20     # aim for jobs ~10-20m; default 20
CPUS_PER_TASK_DEFAULT=4
CONCURRENCY_DEFAULT=50       # limit of concurrent array tasks (tune for I/O)
SBATCH_TIME_DEFAULT="00:25:00" # walltime for each array task (must be >= TARGET_MINUTES; set automatically below)
SBATCH_CPUS_PER_TASK_DEFAULT=4
SBATCH_MEM="12G"
SBATCH_PARTITION=""  # leave empty for default; set if needed e.g. "compute"
SBATCH_EXTRA="#SBATCH --mail-type=END,FAIL" # extra sbatch options, can be overridden via CLI
JOB_NAME_DEFAULT="clip_huc12"
SLURM_LOG_DIR="${CLIPPED_DEM_DIR}/slurm_logs"
#########################################################################

# CLI arg parsing (simple)
TARGET_MINUTES=${TARGET_MINUTES_DEFAULT}
CPUS_PER_TASK=${CPUS_PER_TASK_DEFAULT}
CONCURRENCY=${CONCURRENCY_DEFAULT}
JOB_NAME=${JOB_NAME_DEFAULT}
DRY_RUN=0
while [ $# -gt 0 ]; do
  case "$1" in
    --minutes=*) TARGET_MINUTES="${1#*=}" ;;
    --cpus-per-task=*) CPUS_PER_TASK="${1#*=}" ;;
    --concurrency=*) CONCURRENCY="${1#*=}" ;;
    --job-name=*) JOB_NAME="${1#*=}" ;;
    --dry-run) DRY_RUN=1 ;;
    --help|-h) echo "Usage: $0 [--minutes=N] [--cpus-per-task=N] [--concurrency=N] [--job-name=NAME] [--dry-run]"; exit 0 ;;
    *) echo "Unknown arg: $1"; exit 1 ;;
  esac
  shift
done

# Simple safety checks
if [ ! -f "${MERGED_HUC_FILE}" ]; then
  echo "[ERROR] MERGED_HUC_FILE not found: ${MERGED_HUC_FILE}"
  exit 1
fi
if [ ! -f "${DEM_VRT_FILE}" ]; then
  echo "[ERROR] DEM_VRT_FILE not found: ${DEM_VRT_FILE}"
  exit 1
fi

mkdir -p "${CLIPPED_DEM_DIR}" "${SLURM_LOG_DIR}"

TMPDIR="${CLIPPED_DEM_DIR}/tmp"
CHUNK_DIR="${TMPDIR}/chunks"
STATUS_DIR="${CLIPPED_DEM_DIR}/status"
LOCK_DIR="${CLIPPED_DEM_DIR}/locks"
mkdir -p "${TMPDIR}" "${CHUNK_DIR}" "${STATUS_DIR}" "${LOCK_DIR}"

HUC_LIST="${TMPDIR}/huc12_list.txt"

echo "-------------------------------------------------------------"
echo "Submit clip HUC12 array workflow"
echo "  MERGED_HUC_FILE: ${MERGED_HUC_FILE}"
echo "  DEM_VRT_FILE:    ${DEM_VRT_FILE}"
echo "  OUTPUT_DIR:      ${CLIPPED_DEM_DIR}"
echo "  CHUNK_DIR:       ${CHUNK_DIR}"
echo "  TARGET_MINUTES:  ${TARGET_MINUTES}"
echo "  CPUS_PER_TASK:   ${CPUS_PER_TASK}"
echo "  CONCURRENCY:     ${CONCURRENCY}"
echo "-------------------------------------------------------------"

# Step 1: generate HUC list
echo "[1/6] Generating HUC list..."
# Use ogrinfo output parsing (same method you used)
ogrinfo -ro -q -sql "SELECT ${HUC_ID_FIELD} FROM ${HUC_LAYER_NAME}" "${MERGED_HUC_FILE}" | \
  grep "${HUC_ID_FIELD} (String)" | awk -F' = ' '{print $2}' > "${HUC_LIST}"

NUM_HUCS=$(wc -l < "${HUC_LIST}" | tr -d ' ')
echo "[INFO] Found ${NUM_HUCS} HUC IDs."

if [ "${NUM_HUCS}" -eq 0 ]; then
  echo "[ERROR] HUC list empty. Aborting."
  exit 1
fi

# Step 2: estimate average seconds per file from prior stats (safe floating math via awk)
AVG_SEC_PER_FILE=$(awk -v cores="${PRIOR_CORES}" -v hours="${PRIOR_HOURS}" -v total="${PRIOR_TOTAL_FILES}" 'BEGIN{print (cores*hours*3600)/total}')
# compute target seconds (min -> sec)
TARGET_SECONDS=$(( TARGET_MINUTES * 60 ))
# expected files per core in target duration
# files_per_job = cpus * (target_seconds / avg_sec_per_file)
FILES_PER_JOB_FLOAT=$(awk -v cpus="${CPUS_PER_TASK}" -v tsec="${TARGET_SECONDS}" -v avg="${AVG_SEC_PER_FILE}" 'BEGIN{printf "%f", (cpus * tsec / avg)}')
# ceil to integer, with minimum 1
FILES_PER_JOB=$(awk -v f="${FILES_PER_JOB_FLOAT}" 'BEGIN{f2=int(f); if(f>f2) f2++; if(f2<1) f2=1; print f2}')
# compute number of chunks
NUM_CHUNKS=$(( (NUM_HUCS + FILES_PER_JOB - 1) / FILES_PER_JOB ))

echo "[2/6] Runtime estimate and chunking"
echo "  Historical estimate: ${PRIOR_CORES} cores * ${PRIOR_HOURS} h -> ${PRIOR_TOTAL_FILES} files"
echo "  Avg seconds / file (estimated) = ${AVG_SEC_PER_FILE}"
echo "  Target job length = ${TARGET_MINUTES} minutes (${TARGET_SECONDS} sec)"
echo "  CPUS_PER_TASK = ${CPUS_PER_TASK}"
echo "  Calculated files_per_job (ceil) = ${FILES_PER_JOB} (approx ${FILES_PER_JOB_FLOAT})"
echo "  Will create ${NUM_CHUNKS} chunks (array size)"
echo

# Step 3: split into chunk files (remove old chunks first)
echo "[3/6] Creating chunk files in ${CHUNK_DIR}..."
rm -f "${CHUNK_DIR}"/chunk_*.txt
if [ "${FILES_PER_JOB}" -ge "${NUM_HUCS}" ]; then
  # everything in one chunk
  cp "${HUC_LIST}" "${CHUNK_DIR}/chunk_00.txt"
  NUM_CHUNKS=1
else
  # use split with numeric suffixes; choose width based on NUM_CHUNKS
  WIDTH=$(printf "%02d" "${NUM_CHUNKS}" | wc -c) # approximate width
  # safer: create using split -l
  split -d -l "${FILES_PER_JOB}" --additional-suffix=.txt "${HUC_LIST}" "${CHUNK_DIR}/chunk_"
  # rename to have zero-padded 2-digit suffixes if needed (split produces chunk_00, chunk_01 ... already)
fi
# Count chunks created
ACTUAL_CHUNKS=$(ls -1 "${CHUNK_DIR}"/chunk_*.txt 2>/dev/null | wc -l | tr -d ' ')
if [ "${ACTUAL_CHUNKS}" -eq 0 ]; then
  echo "[ERROR] No chunk files created. Aborting."
  exit 1
fi
echo "[INFO] Chunk files created: ${ACTUAL_CHUNKS}"

# Adjust NUM_CHUNKS to actual
NUM_CHUNKS=${ACTUAL_CHUNKS}

# Step 4: write sbatch array script
SBATCH_SCRIPT="${TMPDIR}/slurm_clip_array.sbatch.sh"
echo "[4/6] Writing sbatch script to ${SBATCH_SCRIPT}..."

# pick a sbatch time slightly larger than TARGET_MINUTES (add safety margin)
# compute SBATCH_TIME automatically: TARGET_MINUTES + 6 minutes margin
MINS_MARGIN=6
SBATCH_MINS=$(( TARGET_MINUTES + MINS_MARGIN ))
# format HH:MM:SS
H=$(( SBATCH_MINS / 60 ))
M=$(( SBATCH_MINS % 60 ))
SBATCH_TIME=$(printf "%02d:%02d:00" "${H}" "${M}")

cat > "${SBATCH_SCRIPT}" <<'SBATCH_EOF'
#!/usr/bin/env bash
#SBATCH --job-name=__JOB_NAME__
#SBATCH --output=__LOG_DIR__/clip_%A_%a.out
#SBATCH --error=__LOG_DIR__/clip_%A_%a.err
#SBATCH --time=__SBATCH_TIME__
#SBATCH --cpus-per-task=__CPUS_PER_TASK__
#SBATCH --mem=__SBATCH_MEM__
__SBATCH_PART__
__SBATCH_EXTRA__
#SBATCH --array=1-__NUM_CHUNKS__%__CONCURRENCY__

set -euo pipefail

# CONFIG (inherited via substitution)
PROCESSED_DIR="__PROCESSED_DIR__"
MERGED_HUC_FILE="__MERGED_HUC_FILE__"
HUC_LAYER_NAME="__HUC_LAYER_NAME__"
HUC_ID_FIELD="__HUC_ID_FIELD__"
DEM_VRT_FILE="__DEM_VRT_FILE__"
CLIPPED_DEM_DIR="__CLIPPED_DEM_DIR__"
TMPDIR="__TMPDIR__"
CHUNK_DIR="__CHUNK_DIR__"
STATUS_DIR="__STATUS_DIR__"
LOCK_DIR="__LOCK_DIR__"

# ensure directories exist
mkdir -p "${CLIPPED_DEM_DIR}" "${TMPDIR}" "${CHUNK_DIR}" "${STATUS_DIR}" "${LOCK_DIR}"

TASK_ID=${SLURM_ARRAY_TASK_ID:-1}
# chunk files are zero-based chunk_00... so task_id - 1 -> file index
IDX=$((TASK_ID-1))
# determine zero-padded suffix width from available files
CHUNK_FILE=$(ls -1 "${CHUNK_DIR}"/chunk_*.txt | sed -n "$((IDX+1))p")
if [ -z "${CHUNK_FILE}" ]; then
  echo "[WARN] Chunk file for index ${IDX} not found; exiting."
  exit 0
fi

echo "[INFO] SLURM task ${TASK_ID} on $(hostname) processing ${CHUNK_FILE}"
echo "[INFO] cpus: ${SLURM_CPUS_ON_NODE:-$SLURM_CPUS_PER_TASK}"

# small randomized sleep to desynchronize startup and reduce I/O spikes
sleep $((RANDOM % 6))

process_huc12() {
  local HUC12_ID="$1"
  local OUT_RASTER="${CLIPPED_DEM_DIR}/${HUC12_ID}.tif"
  local DONE_MARKER="${STATUS_DIR}/${HUC12_ID}.done"
  local LOCK_PATH="${LOCK_DIR}/${HUC12_ID}.lock"
  local TMP_OUT="${TMPDIR}/${HUC12_ID}.tif.$$"

  # skip if done + output exists
  if [ -f "${DONE_MARKER}" ] && [ -f "${OUT_RASTER}" ]; then
    echo "[SKIP] ${HUC12_ID} already done"
    return 0
  fi

  # try to acquire lock (atomic mkdir)
  if ! mkdir "${LOCK_PATH}" 2>/dev/null; then
    echo "[LOCKED] ${HUC12_ID} locked by another process, skipping"
    return 0
  fi
  trap 'rm -rf "${LOCK_PATH}"' RETURN

  # remove stale tmp if any
  rm -f "${TMP_OUT}"

  if gdalwarp -q \
      -cutline "${MERGED_HUC_FILE}" \
      -cl "${HUC_LAYER_NAME}" \
      -cwhere "${HUC_ID_FIELD} = '${HUC12_ID}'" \
      -crop_to_cutline \
      -dstnodata -9999 \
      -co "COMPRESS=LZW" -co "PREDICTOR=2" \
      "${DEM_VRT_FILE}" "${TMP_OUT}"; then

      mv -f "${TMP_OUT}" "${OUT_RASTER}"
      touch "${DONE_MARKER}"
      echo "[DONE] ${HUC12_ID}"
      return 0
  else
      echo "[ERROR] gdalwarp failed for ${HUC12_ID}"
      rm -f "${TMP_OUT}"
      return 1
  fi
}

export -f process_huc12

# prefer GNU parallel if available; otherwise run a simple background-limited loop
CPU_COUNT=${SLURM_CPUS_PER_TASK:-1}
if command -v parallel >/dev/null 2>&1; then
  cat "${CHUNK_FILE}" | parallel -j "${CPU_COUNT}" --halt soon,fail=1 process_huc12 {}
else
  i=0
  for H in $(cat "${CHUNK_FILE}"); do
    process_huc12 "${H}" &
    ((i++))
    if [ "${i}" -ge "${CPU_COUNT}" ]; then
      wait
      i=0
    fi
  done
  wait
fi

echo "[INFO] Task ${TASK_ID} finished."
SBATCH_EOF

# substitute variables into sbatch script
# prepare SBATCH_PART if partition set
if [ -n "${SBATCH_PARTITION}" ]; then
  SBATCH_PART="#SBATCH --partition=${SBATCH_PARTITION}"
else
  SBATCH_PART=""
fi

# now perform substitutions
sed -e "s|__JOB_NAME__|${JOB_NAME}|g" \
    -e "s|__LOG_DIR__|${SLURM_LOG_DIR}|g" \
    -e "s|__SBATCH_TIME__|${SBATCH_TIME}|g" \
    -e "s|__CPUS_PER_TASK__|${CPUS_PER_TASK}|g" \
    -e "s|__SBATCH_MEM__|${SBATCH_MEM}|g" \
    -e "s|__SBATCH_PART__|${SBATCH_PART}|g" \
    -e "s|__SBATCH_EXTRA__|${SBATCH_EXTRA}|g" \
    -e "s|__NUM_CHUNKS__|${NUM_CHUNKS}|g" \
    -e "s|__CONCURRENCY__|${CONCURRENCY}|g" \
    -e "s|__PROCESSED_DIR__|${PROCESSED_DIR}|g" \
    -e "s|__MERGED_HUC_FILE__|${MERGED_HUC_FILE}|g" \
    -e "s|__HUC_LAYER_NAME__|${HUC_LAYER_NAME}|g" \
    -e "s|__HUC_ID_FIELD__|${HUC_ID_FIELD}|g" \
    -e "s|__DEM_VRT_FILE__|${DEM_VRT_FILE}|g" \
    -e "s|__CLIPPED_DEM_DIR__|${CLIPPED_DEM_DIR}|g" \
    -e "s|__TMPDIR__|${TMPDIR}|g" \
    -e "s|__CHUNK_DIR__|${CHUNK_DIR}|g" \
    -e "s|__STATUS_DIR__|${STATUS_DIR}|g" \
    -e "s|__LOCK_DIR__|${LOCK_DIR}|g" \
    "${SBATCH_SCRIPT}" > "${SBATCH_SCRIPT}.final"
chmod +x "${SBATCH_SCRIPT}.final"

# Step 5: final report & dry-run option
echo "[5/6] Summary:"
echo "  Total HUCs:          ${NUM_HUCS}"
echo "  Files per job (est): ${FILES_PER_JOB}"
echo "  Array size:          ${NUM_CHUNKS}"
echo "  SBATCH script:       ${SBATCH_SCRIPT}.final"
echo "  Logs dir:            ${SLURM_LOG_DIR}"
echo "  Status dir:          ${STATUS_DIR}"
echo "  Lock dir:            ${LOCK_DIR}"
echo
echo "  sbatch time per task: ${SBATCH_TIME}"
echo

if [ "${DRY_RUN}" -eq 1 ]; then
  echo "[DRY RUN] Not submitting array. Exiting."
  exit 0
fi

# Step 6: submit sbatch array
echo "[6/6] Submitting SLURM array..."
SUBMIT_CMD="sbatch ${SBATCH_SCRIPT}.final"
echo "[CMD] ${SUBMIT_CMD}"
${SUBMIT_CMD}

echo "Submitted. Use 'squeue -u \$USER' to monitor, and inspect ${SLURM_LOG_DIR} for logs."
echo "You can re-run this script (or just re-submit the sbatch) — completed HUCs are skipped via .done markers."
echo "-------------------------------------------------------------"




================================================
File: .ipynb_checkpoints/download_wbd_zips-checkpoint.sh
================================================
#!/bin/bash
base="https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape"
for i in $(seq -w 1 22); do
  wget -c "$base/WBD_${i}_HU2_Shape.zip"
done



